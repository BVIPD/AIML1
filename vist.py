# -*- coding: utf-8 -*-
"""VIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QIAgWdo40NHVweRXnRun9vNtT_iLJrmJ
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ultralytics/yolov5.git
# %cd yolov5
!pip install -r requirements.txt

# Step 2: Import necessary libraries and load YOLOv5 model
import torch
from PIL import Image
from google.colab import files
from IPython.display import Image as DisplayImage

# Load YOLOv5 model
# Changed the repo_or_dir to '.' to load from the current directory
# which is the root of the yolov5 repository after the %cd command
model = torch.hub.load('.', 'yolov5s', source='local')

# Step 3: Upload an image and run detection
uploaded = files.upload()
img_path = next(iter(uploaded.keys()))  # Get the uploaded image path
img = Image.open(img_path)

# Perform object detection
results = model(img)
results.print()  # Print detected objects and confidence scores
results.show()   # Display image with bounding boxes

# Step 4: Save and display the image with detections
results.save()  # Saves the output in 'runs/detect/exp' folder
output_path = 'runs/detect/exp/' + img_path.split('/')[-1]
DisplayImage(filename=output_path)  # Display the saved output image

# Step 1: Import necessary libraries
import torch
from PIL import Image
from google.colab import files
import os

# Load YOLOv5 model pre-trained to detect faces (if available, else use general object detection)
# Note: Ensure that YOLOv5 is set to detect 'person' or faces
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Alternatively, load a face-detection-specific model if available

# Step 2: Upload a group photo
uploaded = files.upload()
img_path = next(iter(uploaded.keys()))  # Get the uploaded image path
img = Image.open(img_path)

# Step 3: Perform face detection using YOLOv5
results = model(img)
detections = results.pandas().xyxy[0]  # Extract detection results as a DataFrame

# Step 4: Create a directory to save cropped faces
output_dir = "cropped_faces"
os.makedirs(output_dir, exist_ok=True)

# Step 5: Loop through detections, crop each face, and save
for i, row in detections.iterrows():
    # Check if the detected object is a 'person' (for general models) or 'face' (for specific models)
    if row['name'] == 'person':  # Modify to 'face' if using a face-specific model
        # Extract bounding box coordinates
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])

        # Crop the detected face
        face_crop = img.crop((xmin, ymin, xmax, ymax))

        # Save the cropped face
        face_crop_path = os.path.join(output_dir, f"face_{i+1}.jpg")
        face_crop.save(face_crop_path)
        print(f"Saved cropped face to {face_crop_path}")

print(f"All faces cropped and saved in the '{output_dir}' folder.")

pip install opencv-python deepface

# Step 1: Import necessary libraries
import torch
from PIL import Image
from google.colab import files
import os
import cv2
import numpy as np
from deepface import DeepFace
import time
import io
import base64
from IPython.display import display, Javascript, clear_output
from google.colab.output import eval_js
import matplotlib.pyplot as plt

# Load YOLOv5 model pre-trained to detect faces
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Step 2: Upload a group photo for face detection
uploaded = files.upload()
img_path = next(iter(uploaded.keys()))  # Get the uploaded image path
img = Image.open(img_path)

# Step 3: Perform face detection using YOLOv5
results = model(img)
detections = results.pandas().xyxy[0]  # Extract detection results as a DataFrame

# Step 4: Create a directory to save cropped faces
output_dir = "cropped_faces"
os.makedirs(output_dir, exist_ok=True)

# Step 5: Crop and save detected faces from the group photo
for i, row in detections.iterrows():
    if row['name'] == 'person':  # Modify to 'face' if using a face-specific model
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        face_crop = img.crop((xmin, ymin, xmax, ymax))
        face_crop_path = os.path.join(output_dir, f"face_{i+1}.jpg")
        face_crop.save(face_crop_path)
        print(f"Saved cropped face to {face_crop_path}")

print(f"All faces cropped and saved in the '{output_dir}' folder.")

# Step 6: Load all reference images for face verification
reference_dir = '/content/VIST'  # Update this path if necessary
reference_images = []
for filename in os.listdir(reference_dir):
    img_path = os.path.join(reference_dir, filename)
    img = cv2.imread(img_path)
    if img is not None:
        reference_images.append((filename, img))
print(f"Loaded {len(reference_images)} reference images for verification.")

# JavaScript to capture an image from the webcam with permission request
def capture_image():
    js = Javascript('''
    async function takePhoto() {
        // Request webcam access
        try {
            const stream = await navigator.mediaDevices.getUserMedia({video: true});
        } catch (err) {
            console.error('Error accessing webcam:', err);
            return null; // Return null on error
        }

        const video = document.createElement('video');
        video.style.display = 'none';
        document.body.appendChild(video);
        video.srcObject = stream;
        await new Promise((resolve) => (video.onloadedmetadata = resolve));
        video.play();
        await new Promise((resolve) => setTimeout(resolve, 1000));
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        stream.getTracks().forEach((track) => track.stop());
        video.remove();
        return canvas.toDataURL('image/jpeg');
    }
    takePhoto();
    ''')
    display(js)
    data = eval_js("takePhoto();")

    # Handle potential error (permission denied)
    if data is None:
        raise RuntimeError("Webcam access denied. Please grant permission.")

    return data

# Convert the captured image data to OpenCV format
def get_image():
    image_data = capture_image()
    image_bytes = io.BytesIO(base64.b64decode(image_data.split(',')[1]))
    img = Image.open(image_bytes)
    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    return img

# Define a time limit in seconds for the "live" simulation
time_limit = 6

# Step 1: Import necessary libraries
import torch
from PIL import Image
import os
import cv2
import numpy as np
from deepface import DeepFace
import pandas as pd  # For saving matches in an Excel file

# Load YOLOv5 model pre-trained to detect faces
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Step 2: Upload a group photo for face detection
uploaded = files.upload()
img_path = next(iter(uploaded.keys()))  # Get the uploaded image path
img = Image.open(img_path)

# Step 3: Perform face detection using YOLOv5
results = model(img)
detections = results.pandas().xyxy[0]  # Extract detection results as a DataFrame

# Step 4: Create a directory to save cropped faces
output_dir = "cropped_faces"
os.makedirs(output_dir, exist_ok=True)

# Step 5: Crop and save detected faces from the group photo
for i, row in detections.iterrows():
    if row['name'] == 'person':  # Modify to 'face' if using a face-specific model
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        face_crop = img.crop((xmin, ymin, xmax, ymax))
        face_crop_path = os.path.join(output_dir, f"face_{i+1}.jpg")
        face_crop.save(face_crop_path)
        print(f"Saved cropped face to {face_crop_path}")

print(f"All faces cropped and saved in the '{output_dir}' folder.")

# Step 6: Load all reference images for face verification
reference_dir = '/content/VIST'  # Update this path if necessary
reference_images = []
for filename in os.listdir(reference_dir):
    img_path = os.path.join(reference_dir, filename)
    img = cv2.imread(img_path)
    if img is not None:
        reference_images.append((filename, img))
print(f"Loaded {len(reference_images)} reference images for verification.")

# Prepare an empty list to store matched results
matches = []

# Create sets to track already matched cropped faces and used reference images
matched_cropped_faces = set()
used_reference_images = set()

# Loop through each cropped face for verification
for cropped_face_path in os.listdir(output_dir):
    if cropped_face_path in matched_cropped_faces:
        continue  # Skip if this face was already matched

    # Load cropped face image
    cropped_face_img = cv2.imread(os.path.join(output_dir, cropped_face_path))

    # Check the cropped face against each reference image
    face_match = False
    for ref_name, ref_img in reference_images:
        if ref_name in used_reference_images:
            continue  # Skip if this reference image is already matched with another face

        try:
            print(f"Running face verification between {cropped_face_path} and {ref_name}...")
            result = DeepFace.verify(cropped_face_img, ref_img)
            face_match = result['verified']
            print(f"Face match result: {face_match}")

            if face_match:
                # Record the match and add to matched and used sets
                matches.append({'Cropped Face': cropped_face_path, 'Matched Reference': ref_name})
                matched_cropped_faces.add(cropped_face_path)  # Mark as matched
                used_reference_images.add(ref_name)  # Mark this reference as used
                break  # Stop further checks for this face once a match is found
        except ValueError as e:
            print(f"Error in face verification: {e}")
            face_match = False

# Step 7: Save matched results to an Excel file
matches_df = pd.DataFrame(matches)
matches_df.to_excel("matched_faces.xlsx", index=False)
print("Matched results saved to 'matched_faces.xlsx'")